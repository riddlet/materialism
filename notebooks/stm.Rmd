---
title: "MCH"
output: html_notebook
---

First, load in the data and look at a few descriptives.

```{r table1}
library(haven)
library(knitr)
library(dplyr)
library(ggplot2)
library(magrittr)
library(forcats)
df <- read_sav('../data/MCH.sav')
df$Cond <- factor(df$Cond, levels=c('A', 'B', 'C'), labels=c('Low', 'Medium', 'High'))
df %>% 
  filter(!is.na(Attention_Check)) %>%
  group_by(Cond, Attention_Check) %>%
  summarise(number = n()) -> tab
kable(tab, caption = "Number of participants by condition")
```

It looks like people recalling happy purchases were less likely to fail the attention check. Writing about positive purchases makes for more engaged participants, maybe?

```{r fig1, fig.cap = 'Distribution of purchase categories as a function of condition'}
df %<>% filter(`filter_$`==1)
df$Mat_Type1 <- as_factor(df$Mat_Type1)
df %>%
  group_by(Mat_Type1) %>%
  summarise(number_purchases = n()) %>%
  left_join(df) -> df
df %>%
  group_by(Mat_Type1, Cond) %>%
  mutate(number=n()) %>%
  arrange(number_purchases) %>%
  ungroup() %>%
  mutate(Mat_Type1 = fct_reorder(Mat_Type1, number_purchases)) %>%
ggplot(aes(x=Mat_Type1, group=Cond, fill=Cond)) + 
  geom_bar(position = position_dodge()) +
  theme_classic() +
  theme(axis.text.x = element_text(angle=45, hjust=1),
        axis.title.x = element_blank(),
        legend.title = element_blank())
  
```

Looking at the distribution of purchase categories, I think it's pretty clear that people in general were more likely to list either electronics or clothing. Some other informal observations are that it looks like vehicles are killjoy purchases, kitchen appliances elicit a 'meh', jewelry makes people pretty happy, and it's kind of tough to be upset about buying sporting goods. 

To model the actual text, I'm going to use a variant of a Topic model. Topic models are generative statistical models that make assumptions about how a collection of text documents were generated, and then use the observed text to infer the parameters within the statistical model. In general, a topic model consists of words, topics (defined as mixtures of words, where each word has some probability of belonging to the topic), and documents (defined as mixtures of topics, where any given topic has its probability mass distributed across all documents, and the topic probabilities across all documents sums to one).

STM is a variant of the vanilla topic model described above that allows incorporating of meta-information into this generative process. We can specify that the *quantity* of documents that are associated with a topic are dependent upon the metadata (topic prevalence), or we can specify that the *content* of topics are dependent upon the metadata (topic content).

```{r preprocess}
library(stm)
processed <- textProcessor(df$Mat_Essay1, metadata=df)
out <- prepDocuments(processed$documents, processed$vocab, processed$meta)
docs <- out$documents
vocab <- out$vocab
meta <- out$meta
```

Above, we see that the raw corpus has about 45k words. Of these 45k words, we perform some preprocessing to obtain 4,743 unique terms. As seen above, we remove punctuation, numbers, and perform stemming (i.e. convert *running*, *runs*, *runner* all into *run*). Additionally, "stopwords" (e.g. pronouns, determiners, etc) are removed. Although Pennebaker et al. have found that many function words (e.g. pronouns) seem to carry psychological information, here we are more interested in the *content*, and so these common words are removed. Additionally, stripping stop words helps ease the computational load of representing and generating these types of models (though for this particular case, that is not a strong concern). Additionally, all words that occur just once are removed. After this process, we are left with 2,516 distinct terms.

## Topical Prevalence
First, we explore how the condition changes the prevalence of topics across documents, *keeping the content of the topics constant*. Ordinarily, we need to specify how many topics should be fit with a topic model. However, given that we have no *a-priori* reason to pick one value over another, and there is little previous work done using these models for psychological questions, we can take a data-driven approach to evaluate how to pick the number of topics.

There are many metrics one can use to evaluate a topic model. Here, we'll focus on two - the ability of the model to predict new data (held-out likelihood), and the degree to which high probability words are likely to appear in other topics (Exclusivity). For both of these metrics, higher values are better. I fit topic models using $K$ topics where $K=(3, 4, ... 20)$, and show the results of these metrics below.


```{r fig2, fig.cap="Held-out Likelihood as a function of the number of topics"}
prev_fit <- searchK(out$documents, vocab=out$vocab, K=seq(3, 20, by =1), prevalence=~Cond,
                max.em.its = 200, data=out$meta, init.type='Spectral')
ggplot(prev_fit$results, aes(x=K, y=heldout)) + 
  geom_line() + 
  theme_classic() +
  ylab('Held-out Likelihood')
```


```{r fig3, fig.cap="Exclusivity as a function of the number of topics"}
ggplot(prev_fit$results, aes(x=K, y=exclus)) + 
  geom_line() + 
  theme_classic() +
  ylab('Exclusivity')
```


These two metrics suggest around a dozen topics is an ideal point. Held-out likelihood maximizes at k=12 (`r round(pre_fit$results$heldout[12], 2)`), while exclusivity begins to plateau at around K=13 (`r round(pre_fit$results$exclus[13], 2)). After some discussion among my collaborators, we settled on a model with 15 topics. This is estimated below.

```{r, results='hide'}
k15 <- stm(out$documents, vocab=out$vocab, K=15, prevalence=~Cond,
                max.em.its = 200, data=out$meta, init.type='Spectral')
```

After estimating this model, one of the first things to do is look at the word-topic distributions. The STM package provides several methods for examining the "top words" in a topic. `Prob` simply outputs the words that have the highest probability weights in that topic. While straightforward, this has the disadvantage of favoring words that are more common in the corpus. `FREX` weights words by their frequency and how exclusive they are to the topic. `Lift` is similar in that it weights words by how frequently they appear in other topics. `Score` divides the log frequency of the word in the topic by the log frequency of the word in other topics.

```{r}
labelTopics(k15)
```

This illustrates the the estimated topics map reasonably well to many of the purchase categories, though some (e.g. electronics) appear to have split into more than one topic (topic 13, 12, 9).

Let's look at some documents that are strongly associated with specific topics. I'm looking at 15 because I kind of love the essay. I leave it here without comment.

```{r}
findThoughts(k15, texts=df$Mat_Essay1, n=1, topics = 15)
```
This topic appears to be about furniture. This *particular* piece of furniture is a set of dog steps, but I suppose even dogs need furniture...(?).

Anyway, let's examine some other details of the model. The figure below details the overall proportion of all essays that are composed of each of the 15 topics.

```{r}
plot(k15, type='summary', labeltype='frex')
```

Recall that when we looked at the categories chosen as indicated by individuals, clothing and electronics dominated. This is reflected here, where 6 of the 8 top topics come from these two prominent categories (note: the words here are chosen based on their FREX score).

```{r}
prep <- estimateEffect(1:15 ~ Cond, k15, meta=out$meta, uncertainty='Global')
plot(prep, covariate='Cond', topics=10, main='Topic 10: shoe, jean, pair', 
     labeltype='custom', custom.labels=c('Low', 'Medium', 'High'))

```

The figure above shows the relationship between topic 10 and condition. On the x axis is estimated topic proportion, and the bars represent 95% confidence intervals. There does not appear to be a systematic relationship between condition and propensity to write about this particular topic. However, other topics do show some systematic relationship. For instance, topic 10 appears to be a topic that is, to some extent, actually devoted to the "medium" condition. This effect is plotted below, and is reinforced when looking at the top two essays from this topic

```{r}
plot(prep, covariate='Cond', topics=14, main='Topic 14: Scale, Moder, Happy', 
     labeltype='custom', custom.labels=c('Low', 'Medium', 'High'))
```

```{r}
findThoughts(k15, texts=df$Mat_Essay1, n=2, topic=14)
```

I also observe a difference in topic 15:

```{r}
plot(prep, covariate='Cond', topics=15, main='Topic 15: matress, bed, couch', 
     labeltype='custom', custom.labels=c('Low', 'Medium', 'High'))
```

```{r}
findThoughts(k15, texts=df$Mat_Essay1, n=2, topic=15)
```

It is evident that this particular form of analysis tends to pick up on the popularity of writing about various purchases. While this maps well to intuition (it would be odd if a statistical model did *not* pick up on these types of content differences), they hold little theoretical interest. Nonetheless, there are two other topics that show reliable differences. I plot the estimates and sample essays below without elaboration.

```{r}
plot(prep, covariate='Cond', topics=5, main='Topic 5: Kayak, cloth, bra', 
     labeltype='custom', custom.labels=c('Low', 'Medium', 'High'))
```

```{r}
findThoughts(k15, texts=df$Mat_Essay1, n=2, topic=5)
```

```{r}
plot(prep, covariate='Cond', topics=6, main='Topic 6: product, hair, ride', 
     labeltype='custom', custom.labels=c('Low', 'Medium', 'High'))
```

```{r}
findThoughts(k15, texts=df$Mat_Essay1, n=2, topic=6)
```

Because the previous form of model did not capture the question we were interested in, we can now refit a model, allowing topical content (instead of prevalency) to vary by condition. This should allow, in principle, for us to state, for instance, that individuals in condition X wrote about a given topic in a way that is different from those in condition Y. This model is fit below:

```{r}
k15.content <- stm(out$documents, out$vocab, K=15, prevalence=~Cond,
                   content=~Cond, max.em.its = 75,
                   data=out$meta, init.type='Spectral')
```

